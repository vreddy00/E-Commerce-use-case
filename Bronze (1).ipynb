{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f1c9c1b-6e00-4da8-a8d2-aca4c75c2553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./reader_factory/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdf1a58f-6f81-4e50-80a4-9cd2135bcab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.rm(\"abfss://bronze@ecommerceproject1.dfs.core.windows.net/orders\",recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff7f0471-8b1d-467f-9fd1-93e450961717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.rm(\"abfss://stream-data@ecommerceproject1.dfs.core.windows.net/\",recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae862d39-3753-4519-85e7-2f0837c5d618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"dbfs:/FileStore/tables/stream_write/order_year=2016\",\"abfss://stream-data@ecommerceproject1.dfs.core.windows.net/order_year=2016\",recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f2065c-bad3-45fc-b30e-9f1cbd8b0819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"dbfs:/FileStore/tables/stream_write/order_year=2017\",\"abfss://stream-data@ecommerceproject1.dfs.core.windows.net/order_year=2017\",recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a9ec0e8-2831-4879-a577-97123a9aae95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"dbfs:/FileStore/tables/stream_write/order_year=2018\",\"abfss://stream-data@ecommerceproject1.dfs.core.windows.net/order_year=2018\",recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab500ff9-9c77-4ed2-81f1-143138b8a4f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Reading Batch Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c62e458-72a8-4b1c-847a-06105bed026b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Extractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def extract(self):\n",
    "        pass\n",
    "\n",
    "class Order_items(Extractor):\n",
    "    def extract(self):\n",
    "        delta_table_path = \"abfss://watermark@ecommerceproject1.dfs.core.windows.net/metadata/processed_timestamp\"\n",
    "        try:\n",
    "            df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "            last_processed_timestamp = df.select(\"last_processed_timestamp\").collect()[0][0]\n",
    "            print(\"Last Processed Timestamp:\", last_processed_timestamp)\n",
    "        except:\n",
    "            last_processed_timestamp = 0  # Default to 0 if table doesn't exist\n",
    "        files = dbutils.fs.ls(\"abfss://batch-data@ecommerceproject1.dfs.core.windows.net/olist_order_items_dataset/\")\n",
    "        new_files = [f for f in files if f.modificationTime > last_processed_timestamp]\n",
    "        if new_files:\n",
    "            print(\"New files found:\", [f.path for f in new_files])\n",
    "        else:\n",
    "            print(\"No new files found.\")\n",
    "            return {}\n",
    "        latest_mod_time = max(f.modificationTime for f in new_files)\n",
    "        latest_files = [f for f in new_files if f.modificationTime == latest_mod_time]\n",
    "        order_itemsDf = None\n",
    "        for file in latest_files:\n",
    "            order_itemsDf = get_DataSource(\"csv\", file.path).getDataFrame()\n",
    "            break  \n",
    "        inputDf = {\"order_itemsDf\": order_itemsDf}\n",
    "        spark.createDataFrame([(latest_mod_time,)], [\"last_processed_timestamp\"]) \\\n",
    "            .write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "        print(\"Updated Last Processed Timestamp:\", latest_mod_time)\n",
    "        return inputDf\n",
    "    \n",
    "class Customers(Extractor):\n",
    "    def extract(self):\n",
    "        customerDf = get_DataSource(\"csv\",\"abfss://batch-data@ecommerceproject1.dfs.core.windows.net/olist_customers_dataset.csv\").getDataFrame()\n",
    "        \n",
    "        inputDf = {\"customerDf\":customerDf}\n",
    "\n",
    "        return inputDf\n",
    "    \n",
    "class Geolocation(Extractor):\n",
    "    def extract(self):\n",
    "        geolocationDf = get_DataSource(\"csv\",\"abfss://batch-data@ecommerceproject1.dfs.core.windows.net/olist_geolocation_dataset.csv\").getDataFrame()\n",
    "        \n",
    "        inputDf = {\"geolocationDf\":geolocationDf}\n",
    "\n",
    "        return inputDf\n",
    "    \n",
    "class Products(Extractor):\n",
    "    def extract(self):\n",
    "        products_Df = get_DataSource(\"csv\",\"abfss://batch-data@ecommerceproject1.dfs.core.windows.net/olist_products_dataset.csv\").getDataFrame()\n",
    "\n",
    "        inputDf = {\"products_Df\":products_Df}\n",
    "\n",
    "        return inputDf\n",
    "\n",
    "class Products_Category(Extractor):\n",
    "    def extract(self):\n",
    "        categroy_translation_Df = get_DataSource(\"csv\",\"abfss://batch-data@ecommerceproject1.dfs.core.windows.net/product_category_name_translation.csv\").getDataFrame()\n",
    "        \n",
    "        inputDf = {\"categroy_translation_Df\":categroy_translation_Df}\n",
    "\n",
    "        return inputDf\n",
    "    \n",
    "class Sellers(Extractor):\n",
    "    def extract(self):\n",
    "        sellerDf = get_DataSource(\"csv\",\"abfss://batch-data@ecommerceproject1.dfs.core.windows.net/olist_sellers_dataset.csv\").getDataFrame()\n",
    "        \n",
    "        inputDf = {\"sellerDf\":sellerDf}\n",
    "\n",
    "        return inputDf\n",
    "    \n",
    "class Payments(Extractor):\n",
    "    def extract(self):\n",
    "        paymentDf = get_DataSource(\"csv\",\"abfss://batch-data@ecommerceproject1.dfs.core.windows.net/olist_order_payments_dataset.csv\").getDataFrame()\n",
    "        \n",
    "        inputDf = {\"paymentDf\":paymentDf}\n",
    "\n",
    "        return inputDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a866562-c94d-4f34-af21-1c00259ba659",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, col\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "class StreamingProcessor:\n",
    "    def __init__(self, source_path, bronze_table_path):\n",
    "        self.source_path = source_path\n",
    "        self.bronze_table_path = bronze_table_path\n",
    "        self.delta_table_path = bronze_table_path + \"/delta_table/\"\n",
    "        self.schema_checkpoint = bronze_table_path + \"/schema/\"\n",
    "        self.bronze_checkpoint = bronze_table_path + \"/checkpoints/bronze/\"\n",
    "\n",
    "    def read_streaming_data(self):\n",
    "        # Step 1: Read Incremental Data using AutoLoader\n",
    "        streaming_df = (spark.readStream\n",
    "            .format(\"cloudFiles\")\n",
    "            .option(\"cloudFiles.format\", \"csv\")\n",
    "            .option(\"cloudFiles.schemaLocation\", self.schema_checkpoint)\n",
    "            .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "            .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .load(self.source_path))\n",
    "        return streaming_df\n",
    "\n",
    "    def partition_streaming_data(self, df):\n",
    "        # Step 2: Add Partition Columns (Year & Month) for Storage Optimization\n",
    "        partitioned_df = (df\n",
    "            .withColumn(\"order_year\", year(col(\"order_purchase_timestamp\")))\n",
    "            .withColumn(\"order_month\", month(col(\"order_purchase_timestamp\"))))\n",
    "        return partitioned_df\n",
    "\n",
    "    def upsert_to_bronze(self, micro_batch_df, batch_id):\n",
    "        # Step 3: Upsert Function to Merge Data into Bronze Table\n",
    "        if not DeltaTable.isDeltaTable(spark, self.delta_table_path):\n",
    "            print(\" Bronze Delta table not found. Creating a new one...\")\n",
    "            micro_batch_df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"order_year\", \"order_month\").save(self.delta_table_path)\n",
    "        else:\n",
    "            print(f\" Batch ID {batch_id}: Performing upsert...\")\n",
    "            bronze_table = DeltaTable.forPath(spark, self.delta_table_path)\n",
    "            (bronze_table.alias(\"bronze\")\n",
    "                .merge(micro_batch_df.alias(\"source\"),\n",
    "                       \"bronze.order_id = source.order_id\")\n",
    "                .whenMatchedUpdateAll()\n",
    "                .whenNotMatchedInsertAll()\n",
    "                .execute())\n",
    "\n",
    "    def start_streaming_job(self):\n",
    "        # Initialize Streaming and Upsert Logic\n",
    "        streaming_df = self.read_streaming_data()\n",
    "        partitioned_df = self.partition_streaming_data(streaming_df)\n",
    "        upsert_query = (partitioned_df.writeStream\n",
    "            .foreachBatch(self.upsert_to_bronze)\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"checkpointLocation\", self.bronze_checkpoint)\n",
    "            .trigger(availableNow=True)\n",
    "            .start())\n",
    "        return upsert_query\n",
    "\n",
    "# # Initialize the Streaming Processor\n",
    "source_path = \"abfss://stream-data@ecommerceproject1.dfs.core.windows.net/\"\n",
    "bronze_table_path = \"abfss://bronze@ecommerceproject1.dfs.core.windows.net/orders\"\n",
    " \n",
    "# stream_processor = StreamingProcessor(source_path, bronze_table_path)\n",
    "\n",
    "# # Start the streaming job\n",
    "# streaming_query = stream_processor.start_streaming_job()\n",
    "# streaming_query.awaitTermination(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f34d629-36a5-40d2-88cb-46cad89378ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Read Streaming Incremental Data using AutoLoader**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "627a1699-9ede-43e4-8de9-aa1476ab8db6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbab9de9-fcd3-4e49-8531-5677cd151d94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2652645158221277,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze (1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
