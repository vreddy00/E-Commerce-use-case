{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4db71e90-6a37-4ff2-b960-080f8a35cf41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead,col,broadcast,collect_set,size,array_contains,collect_list,min,datediff, avg,rank,when,count,coalesce,lit,desc,sum,countDistinct,udf,date_format\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d5f401f-8ee4-4443-8ff4-e6edc6878d9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Bronze/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "552cd084-1905-4f4c-9a9e-baba8a16c538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class IncrementalPreProcessor:\n",
    "    def __init__(self, bronze_table_path, silver_table_path):\n",
    "        self.bronze_table_path = bronze_table_path\n",
    "        self.silver_table_path = silver_table_path\n",
    "        self.checkpoint_path =  silver_table_path + \"/checkpoint/\"\n",
    "\n",
    "    def preprocess_stream(self):\n",
    "        # Step 1: Read from Bronze Delta table as a Stream\n",
    "        bronze_stream = (spark.readStream\n",
    "            .format(\"delta\")\n",
    "            .load(self.bronze_table_path))\n",
    "\n",
    "        # Step 2: Apply Preprocessing (drop columns, fill nulls, etc.)\n",
    "        processed_stream = (bronze_stream\n",
    "            .drop(\"order_approved_at\", \"order_delivered_carrier_date\", \"order_delivered_customer_date\", \"order_estimated_delivery_date\",\"_rescued_data\")\n",
    "            .fillna({\"order_status\": \"unavailable\"}) \n",
    "        )\n",
    "\n",
    "        # Step 3: Write preprocessed data to Silver Delta Table\n",
    "        query = (processed_stream.writeStream\n",
    "            .format(\"delta\")\n",
    "            .outputMode(\"append\")\n",
    "            .queryName(\"Preprocessing Stream data\")\n",
    "            .option(\"checkpointLocation\", self.checkpoint_path)\n",
    "            .start(self.silver_table_path))\n",
    "\n",
    "        return query\n",
    "\n",
    "# # Paths for Delta tables\n",
    "# bronze_table_path = \"abfss://bronze@ecommerceproject.dfs.core.windows.net/orders/delta_table\"\n",
    "# silver_table_path = \"abfss://silver@ecommerceproject.dfs.core.windows.net/orders\"\n",
    "\n",
    "# # Start Streaming Preprocessing\n",
    "# processor = IncrementalPreProcessor(bronze_table_path, silver_table_path)\n",
    "# query = processor.preprocess_stream()\n",
    "# query.awaitTermination(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7167471-8872-40c3-8eeb-4054b1a94747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# create schema if not exists eco.silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9846ab05-76b3-481b-a550-0ab3226672d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class PreProcessPipeline:\n",
    "    def __init__(self, storage_base_path: str):\n",
    "        self.storage_base_path = storage_base_path.rstrip('/')  # Remove trailing slash if any.\n",
    "\n",
    "    def preprocess_data(self, df: DataFrame, drop_columns: list = None, fill_na_dict: dict = None) -> DataFrame:\n",
    "        if drop_columns:\n",
    "            print(f\"Dropping columns: {drop_columns}\")\n",
    "            df = df.drop(*drop_columns)\n",
    "        if fill_na_dict:\n",
    "            print(f\"Filling nulls with: {fill_na_dict}\")\n",
    "            df = df.fillna(fill_na_dict)\n",
    "        return df\n",
    "\n",
    "    def store_as_delta(self, df: DataFrame, table_name: str):\n",
    "        delta_table_path = f\"{self.storage_base_path}/{table_name}\"\n",
    "        try:\n",
    "            df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "            print(f\" Data stored as Delta table at {delta_table_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\" Error storing Delta table at {delta_table_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run_pipeline(self, df: DataFrame, table_name: str, drop_columns: list = None, fill_na_dict: dict = None):\n",
    "\n",
    "        print(f\" Starting PreProcess pipeline for table: {table_name}\")\n",
    "        \n",
    "        # Step 1: Preprocess data (drop columns and fill nulls)\n",
    "        processed_df = self.preprocess_data(df, drop_columns, fill_na_dict)\n",
    "        \n",
    "        # Define the Delta table path in storage.\n",
    "        delta_table_path = f\"{self.storage_base_path}/{table_name}\"\n",
    "        \n",
    "        # Step 2: Check if Delta table exists in storage; if not, store the DataFrame.\n",
    "        if not DeltaTable.isDeltaTable(spark, delta_table_path):\n",
    "            print(f\"Delta table not found at {delta_table_path}. Storing data as Delta table.\")\n",
    "            self.store_as_delta(processed_df, table_name)\n",
    "        else:\n",
    "            print(f\"Delta table already exists at {delta_table_path}. Skipping store as delta step.\")\n",
    "\n",
    "        print(f\" Pre-Processing pipeline completed for table: {table_name}\")\n",
    "\n",
    "storage_base_path = \"abfss://silver@ecommerceproject.dfs.core.windows.net/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "67c8b3cb-fef0-4c8d-a36b-97e470c227ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_order_items(inputDf: dict) -> DataFrame:\n",
    "\n",
    "    # Define configuration for the order_items table.\n",
    "    config = {\n",
    "        \"table_name\": \"order_items\",                  \n",
    "        \"drop_columns\": [\"seller_id\", \"shipping_limit_date\"],\n",
    "        \"fill_na_dict\": {\"price\": 0.0, \"freight_value\": 0.0}\n",
    "    }\n",
    "    \n",
    "    # Initialize the PreProcess pipeline.\n",
    "    pipeline = PreProcessPipeline(storage_base_path)\n",
    "    \n",
    "    # Run the pipeline only once.\n",
    "    pipeline.run_pipeline(inputDf.get(\"order_itemsDf\"), config[\"table_name\"], config[\"drop_columns\"], config[\"fill_na_dict\"])\n",
    "\n",
    "    # Read the preprocessed table from ADLS\n",
    "    delta_table_path = f\"{storage_base_path}/{config['table_name']}\"\n",
    "    print(f\"Loading preprocessed table from Delta path: {delta_table_path}\")\n",
    "    return spark.read.format(\"delta\").load(delta_table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "44d5e523-fef5-4190-b660-84e570d520d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_products(inputDf: dict) -> DataFrame:\n",
    "\n",
    "    # Define configuration for the products table.\n",
    "    config = {\n",
    "        \"table_name\": \"products\",                  \n",
    "        \"drop_columns\": [\"product_name_lenght\",\"product_description_lenght\", \"product_photos_qty\",\"product_weight_g\",\"product_length_cm\", \"product_height_cm\",\"product_width_cm\"],\n",
    "        \"fill_na_dict\": {\"product_category_name\": \"unknown\"}\n",
    "    }\n",
    "    \n",
    "    # Initialize the PreProcess pipeline.\n",
    "    pipeline = PreProcessPipeline(storage_base_path)\n",
    "    \n",
    "    # Run the pipeline only once.\n",
    "    pipeline.run_pipeline(inputDf.get(\"products_Df\"), config[\"table_name\"], config[\"drop_columns\"], config[\"fill_na_dict\"])\n",
    "    \n",
    "    # Read the preprocessed table from ADLS\n",
    "    delta_table_path = f\"{storage_base_path}/{config['table_name']}\"\n",
    "    print(f\"Loading preprocessed table from Delta path: {delta_table_path}\")\n",
    "    return spark.read.format(\"delta\").load(delta_table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "28169f67-9f79-4115-ac3a-51d3631e37da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_preprocessed_customers(inputDf: dict) -> DataFrame:\n",
    "\n",
    "    # Define configuration for the products table.\n",
    "    config = {\n",
    "        \"table_name\": \"customers\",                  \n",
    "        \"drop_columns\": [\"customer_unique_id\"],\n",
    "        \"fill_na_dict\": {}\n",
    "    }\n",
    "    \n",
    "    # Initialize the PreProcess pipeline.\n",
    "    pipeline = PreProcessPipeline(storage_base_path)\n",
    "    \n",
    "    # Run the pipeline only once.\n",
    "    pipeline.run_pipeline(inputDf.get(\"customerDf\"), config[\"table_name\"], config[\"drop_columns\"], config[\"fill_na_dict\"])\n",
    "    \n",
    "    # Read the preprocessed table from ADLS\n",
    "    delta_table_path = f\"{storage_base_path}/{config['table_name']}\"\n",
    "    print(f\"Loading preprocessed table from Delta path: {delta_table_path}\")\n",
    "    return spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bb253171-2d7e-433b-8c25-91bb50a150fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_product_category(inputDf: dict) -> DataFrame:\n",
    "\n",
    "    # Define configuration for the products table.\n",
    "    config = {\n",
    "        \"table_name\": \"product_category\",                  \n",
    "        \"drop_columns\": [],\n",
    "        \"fill_na_dict\": {}\n",
    "    }\n",
    "    \n",
    "    # Initialize the PreProcess pipeline.\n",
    "    pipeline = PreProcessPipeline(storage_base_path)\n",
    "    \n",
    "    # Run the pipeline only once.\n",
    "    pipeline.run_pipeline(inputDf.get(\"categroy_translation_Df\"), config[\"table_name\"], config[\"drop_columns\"], config[\"fill_na_dict\"])\n",
    "    \n",
    "    # Read the preprocessed table from ADLS\n",
    "    delta_table_path = f\"{storage_base_path}/{config['table_name']}\"\n",
    "    print(f\"Loading preprocessed table from Delta path: {delta_table_path}\")\n",
    "    return spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "24b5f1c6-7952-479f-b615-f0e2a8a71fd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_geolocation(inputDf: dict) -> DataFrame:\n",
    "    replacements = {\n",
    "        'á': 'a', 'à': 'a', 'ã': 'a', 'â': 'a', 'ä': 'a',\n",
    "        'é': 'e', 'è': 'e', 'ê': 'e',\n",
    "        'í': 'i', 'ì': 'i', 'î': 'i',\n",
    "        'ó': 'o', 'ò': 'o', 'ô': 'o', 'õ': 'o', 'ö': 'o',\n",
    "        'ú': 'u', 'ù': 'u', 'ü': 'u',\n",
    "        'ç': 'c', 'ñ': 'n'\n",
    "    }\n",
    "\n",
    "    # Function to replace accented characters\n",
    "    def replace_accented(text):\n",
    "        if text:\n",
    "            return re.sub(r'[áàãâäéèêíìîóòôõöúùüçñ]', lambda match: replacements.get(match.group(0), match.group(0)), text)\n",
    "        return text\n",
    "\n",
    "    # Convert function to PySpark UDF\n",
    "    replace_accented_udf = udf(replace_accented, StringType())\n",
    "\n",
    "    # Read geolocation dataset\n",
    "    df = inputDf.get(\"geolocationDf\")\n",
    "\n",
    "    # Drop duplicate zip codes (keeping only one row per zip code)\n",
    "    df_unique = df.dropDuplicates([\"geolocation_zip_code_prefix\"])\n",
    "\n",
    "    # Standardize city names\n",
    "    df_cleaned = df_unique.withColumn(\"cleaned_geolocation_city\", replace_accented_udf(col(\"geolocation_city\")))\n",
    "\n",
    "    # Define preprocessing configuration\n",
    "    config = {\n",
    "        \"table_name\": \"geolocation_cleaned\",\n",
    "        \"drop_columns\": [\"geolocation_lat\",\"geolocation_lng\",\"geolocation_city\"], \n",
    "        \"fill_na_dict\": {}  \n",
    "    }\n",
    "\n",
    "    # Initialize preprocessing pipeline\n",
    "    pipeline = PreProcessPipeline(storage_base_path)\n",
    "\n",
    "    # Run the pipeline\n",
    "    pipeline.run_pipeline(df_cleaned, config[\"table_name\"], config[\"drop_columns\"], config[\"fill_na_dict\"])\n",
    "\n",
    "    # Read the preprocessed table from ADLS\n",
    "    delta_table_path = f\"{storage_base_path}/{config['table_name']}\"\n",
    "    print(f\"Loading preprocessed table from Delta path: {delta_table_path}\")\n",
    "    return spark.read.format(\"delta\").load(delta_table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c70839b7-384b-4bff-a9c9-80d41cbbb6a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Transformer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def transform(self, inputDf):\n",
    "        raise NotImplementedError(\"transform() not implemented\")\n",
    "\n",
    "class TotalRevenueTransformer(Transformer):\n",
    "    \"\"\"Total revenue from orders\"\"\"\n",
    "    def transform(self, preprocessed_df: DataFrame):\n",
    "        # Now preprocessed_df is the deep-cloned table loaded from Unity Catalog.\n",
    "        transformed_df = preprocessed_df.withColumn(\"Item_Total\", col(\"price\") + col(\"freight_value\"))\n",
    "        revenue_df = transformed_df.groupby(\"order_id\").agg(sum(\"Item_Total\").alias(\"Total_price\"))\n",
    "        print(\"Total revenue per order:\")\n",
    "        revenue_df.orderBy(\"Total_price\", ascending=False).display()\n",
    "        rdf=revenue_df.select(sum(\"Total_price\").alias(\"total_revenue\"))\n",
    "        rdf.display()\n",
    "        return rdf\n",
    "    \n",
    "class RevenueByProductCategoryTransformer(Transformer):\n",
    "    \"\"\"Total revenue from each product category\"\"\"\n",
    "    def transform(self, joined_df: DataFrame):\n",
    "        # Now joined_df is the join of order_items,products and product_category tables.\n",
    "        transformed_df = joined_df.withColumn(\"Item_Total\", col(\"price\") + col(\"freight_value\"))\n",
    "        revenue_df = transformed_df.groupby(\"product_category_name_english\").agg(sum(\"Item_Total\").alias(\"Category_Revenue\"))\n",
    "        print(\"Total revenue per category:\")\n",
    "        rdf=revenue_df.orderBy(\"Category_Revenue\", ascending=False)\n",
    "        #revenue_df.select(sum(\"Total_price\").alias(\"total_revenue\")).display()\n",
    "        rdf.display()\n",
    "        return rdf\n",
    "    \n",
    "class TopSellingProduct(Transformer):\n",
    "    \"\"\"Top Selling Products\"\"\"\n",
    "    def transform(self, preprocessed_df: DataFrame):\n",
    "        # Use the same preprocessed table.\n",
    "        transformed_df = preprocessed_df.withColumn(\"Item_Total\", col(\"price\") + col(\"freight_value\"))\n",
    "        result_df = transformed_df.groupby(\"product_id\").agg(count(\"order_item_id\").alias(\"order_count\"))\n",
    "        print(\"Top Selling Products (by order count):\")\n",
    "        rdf=result_df.orderBy(desc(\"order_count\"))\n",
    "        rdf.display()\n",
    "        return rdf\n",
    "\n",
    "class RevenueTrendOverTimeTransformer(Transformer):\n",
    "    \"\"\"Daily revenue from orders\"\"\"\n",
    "    def transform(self, joined_df: DataFrame):\n",
    "        # Now preprocessed_df is the deep-cloned table loaded from Unity Catalog.\n",
    "        transformed_df = joined_df.withColumn(\"Item_Total\", col(\"price\") + col(\"freight_value\"))\n",
    "        revenue_df = transformed_df.groupby(date_format(\"order_purchase_timestamp\", \"yyyy-MM-dd\").alias(\"date\")).agg(sum(\"Item_Total\").alias(\"Daily_Revenue\"))\n",
    "        print(\"Daily revenue :\")\n",
    "        rdf=revenue_df.orderBy(\"Daily_Revenue\", ascending=False)\n",
    "        # revenue_df.select(sum(\"Total_price\").alias(\"total_revenue\")).display()\n",
    "        rdf.display()\n",
    "        return rdf\n",
    "    \n",
    "class GeographicRevenueTransformer(Transformer):\n",
    "    \"\"\"State wise revenue from orders\"\"\"\n",
    "    def transform(self, joined_df: DataFrame):\n",
    "        # Now preprocessed_df is the deep-cloned table loaded from Unity Catalog.\n",
    "        transformed_df = joined_df.withColumn(\"Item_Total\", col(\"price\") + col(\"freight_value\"))\n",
    "        revenue_df = transformed_df.groupby(\"customer_state\").agg(sum(\"Item_Total\").alias(\"State_Revenue\"))\n",
    "        print(\"State wise revenue :\")\n",
    "        rdf=revenue_df.orderBy(\"State_Revenue\", ascending=False)\n",
    "        rdf.display()\n",
    "        return rdf\n",
    "    \n",
    "class CustomerValueTransformer(Transformer):\n",
    "    \"\"\"State wise revenue from orders\"\"\"\n",
    "    def transform(self, joined_df: DataFrame):\n",
    "        # Now preprocessed_df is the deep-cloned table loaded from Unity Catalog.\n",
    "        transformed_df = joined_df.withColumn(\"Item_Total\", col(\"price\") + col(\"freight_value\"))\n",
    "        revenue_df = transformed_df.groupby(\"customer_id\").agg(sum(\"Item_Total\").alias(\"Customer_Spent\"))\n",
    "        revenue_df = revenue_df.withColumn(\n",
    "            \"Customer_Category\",\n",
    "            when((col(\"Customer_Spent\") >= 5001), \"High\")\n",
    "            .when((col(\"Customer_Spent\") >= 2001) & (col(\"Customer_Spent\") <= 5000), \"Mid\")\n",
    "            .when((col(\"Customer_Spent\") >= 0) & (col(\"Customer_Spent\") <= 2000), \"Low\")\n",
    "            .otherwise(\"Unknown\")\n",
    "        )\n",
    "        print(\"Customer Revenue and Category:\")\n",
    "        rdf=revenue_df.orderBy(col(\"Customer_Spent\").desc())\n",
    "        rdf.display()\n",
    "        return rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b6b0706-726a-4366-9676-a6087711d707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.streaming.statefulOperator.checkCorrectness.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d91111e5-63b6-4466-ae1c-2e53a3b96d32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.streaming import StreamingQuery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22fe7c11-22a7-4c19-8c11-1413ef1b97c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_stream_static_join(stream_table_name: DataFrame, static_table_name: DataFrame, stream_join_column: str, static_join_column: str,output_table_name: str, output_checkpoint_path: str) -> 'StreamingQuery':\n",
    "    # Step 1: Read streaming data from Unity Catalog\n",
    "    stream_df =stream_table_name\n",
    "\n",
    "    # Step 2: Read static batch data from Unity Catalog\n",
    "    static_df = static_table_name\n",
    "\n",
    "    # Step 3: Perform the join (default shuffle join)\n",
    "    joined_stream = (stream_df\n",
    "        .join(static_df, stream_df[stream_join_column] == static_df[static_join_column], \"left\") ) # Left join\n",
    "\n",
    "    # Step 4: Select all columns from stream_df and all columns from static_df except the join column\n",
    "    selected_columns = [stream_df[col] for col in stream_df.columns] + \\\n",
    "                       [static_df[col] for col in static_df.columns if col != static_join_column]\n",
    "\n",
    "    joined_stream = joined_stream.select(*selected_columns)\n",
    "\n",
    "    # Step 4: Write the result to a Delta table with checkpointing\n",
    "    query = (joined_stream.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", output_checkpoint_path)\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .start(output_table_name) )\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "201846a0-f9d1-453f-83de-36a332bad61b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def productwise_revenue_join(order_items,products,product_categories):\n",
    "    joined_df = order_items.join(products, order_items.product_id == products.product_id) \\\n",
    "                       .join(product_categories, products.product_category_name == product_categories.product_category_name)\\\n",
    "                           .select(order_items[\"order_id\"], order_items[\"order_item_id\"], order_items[\"product_id\"], order_items[\"price\"], order_items[\"freight_value\"],products[\"product_category_name\"],product_categories[\"product_category_name_english\"])\n",
    "\n",
    "    joined_df.write.format(\"delta\").mode(\"overwrite\").save(\"abfss://gold@ecommerceproject.dfs.core.windows.net/joins/productwise_revenue/delta_table\")\n",
    "\n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8736fa87-af7d-4eef-8dba-75c4d3ca11aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def order_items_join(order_df: DataFrame, order_items_df: DataFrame) -> DataFrame:\n",
    "    stream_table_name =order_df\n",
    "    static_table_name = order_items_df\n",
    "    stream_join_column = \"order_id\" \n",
    "    static_join_column = \"order_id\" \n",
    "    output_table_name = \"abfss://gold@ecommerceproject.dfs.core.windows.net/joins/orders_items_join/delta_table\" \n",
    "    output_checkpoint_path = \"abfss://gold@ecommerceproject.dfs.core.windows.net/joins/orders_items_join/checkpoint\" \n",
    "    query = create_stream_static_join(stream_table_name, static_table_name, stream_join_column,static_join_column, output_table_name, output_checkpoint_path)\n",
    "    \n",
    "    # spark.sql(\"create table if not exists eco.gold.orders_items_join location 'abfss://gold@ecommerceproject.dfs.core.windows.net/joins/orders_items_join/delta_table'\")\n",
    "    query.awaitTermination(10)\n",
    "    joined_stream_df = spark.readStream.format(\"delta\").load(\"abfss://gold@ecommerceproject.dfs.core.windows.net/joins/orders_items_join/delta_table\")\n",
    "    return joined_stream_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0835ab05-9772-46ec-b1c3-544a6f429509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def customer_spending_join(orders:DataFrame,order_items:DataFrame,customers:DataFrame) -> DataFrame:\n",
    "    order_items_join(orders,order_items)\n",
    "    stream_table_name = order_items_join(orders,order_items) \n",
    "    static_table_name = customers  \n",
    "    stream_join_column = \"customer_id\" \n",
    "    static_join_column = \"customer_id\" \n",
    "    output_table_name = \"abfss://gold@ecommerceproject.dfs.core.windows.net/joins/customer_spending/delta_table\"  \n",
    "    output_checkpoint_path = \"abfss://gold@ecommerceproject.dfs.core.windows.net/joins/customer_spending/checkpoint\" \n",
    "    query = create_stream_static_join(stream_table_name, static_table_name,stream_join_column,static_join_column, output_table_name, output_checkpoint_path) \n",
    "    # spark.sql(\"create table if not exists eco.gold.customer_spending location 'abfss://gold@ecomadls.dfs.core.windows.net/joins/customer_spending/delta_table'\")\n",
    "    query.awaitTermination(30)\n",
    "    joined_stream_df = spark.readStream.format(\"delta\").load(\"abfss://gold@ecommerceproject.dfs.core.windows.net/joins/customer_spending/delta_table\")\n",
    "    return joined_stream_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b75bca96-0954-4266-aa9c-3d0748b50647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# source_path = \"abfss://stream-data@ecommerceproject.dfs.core.windows.net/\"\n",
    "# bronze_table_path = \"abfss://bronze@ecommerceproject.dfs.core.windows.net/orders\"\n",
    "# delta_table_path=\"abfss://bronze@ecommerceproject.dfs.core.windows.net/orders/delta_table/\"\n",
    "# silver_table_path = \"abfss://silver@ecommerceproject.dfs.core.windows.net/orders/\"\n",
    "\n",
    "# orderi_extractor = Order_items()\n",
    "# orderitemDf = orderi_extractor.extract()\n",
    "\n",
    "# cust_extractor=Customers()\n",
    "# customerDf = cust_extractor.extract()\n",
    "\n",
    "# extractor = StreamingProcessor(source_path, bronze_table_path)\n",
    "# orderDf = extractor.start_streaming_job()\n",
    "# orderDf.awaitTermination(30)\n",
    "\n",
    "\n",
    "#         #Tranformation: Preprocessing & Joins\n",
    "# preprocessed_order_items_df = get_preprocessed_order_items(orderitemDf)\n",
    "# preprocessed_customer_df = get_preprocessed_customers(customerDf)\n",
    "# preprocessed_order_df=IncrementalPreProcessor(delta_table_path, silver_table_path)\n",
    "# query=preprocessed_order_df.preprocess_stream()\n",
    "# query.awaitTermination(30)\n",
    "# preprocessed_order_df=spark.readStream.format(\"delta\").load(silver_table_path)\n",
    "#         #Transformation:Join\n",
    "# joined_df = customer_spending_join(preprocessed_order_df,preprocessed_order_items_df,preprocessed_customer_df)\n",
    "# # stream_joindf=spark.readStream.format(\"delta\").load(\"abfss://gold@ecomadls.dfs.core.windows.net/customer_spending/delta_table\")\n",
    "\n",
    "#         #Transformation: Aggregation\n",
    "# customer_value_transformer = CustomerValueTransformer()\n",
    "# revenue_result = customer_value_transformer.transform(joined_df)\n",
    "# display(revenue_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acc7893e-cf44-4b4f-87a4-1bdc8dff6b48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# source_path = \"abfss://stream-data@ecommerceproject.dfs.core.windows.net/\"\n",
    "# bronze_table_path = \"abfss://bronze@ecommerceproject.dfs.core.windows.net/orders\"\n",
    "# delta_table_path=\"abfss://bronze@ecommerceproject.dfs.core.windows.net/orders/delta_table/\"\n",
    "# silver_table_path = \"abfss://silver@ecommerceproject.dfs.core.windows.net/orders/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "168c1737-a6e3-418b-9107-a8d74e568254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def geographic_revenue_join(orders,order_items,customers,geolocation):\n",
    "    stream_table_name = customer_spending_join(orders,order_items,customers)\n",
    "    static_table_name = geolocation  \n",
    "    stream_join_column = \"customer_zip_code_prefix\" \n",
    "    static_join_column = \"geolocation_zip_code_prefix\" \n",
    "    output_table_name = \"abfss://gold@ecommerceproject.dfs.core.windows.net/joins/geographic_revenue/delta_table\"  \n",
    "    output_checkpoint_path = \"abfss://gold@ecommerceproject.dfs.core.windows.net/joins/geographic_revenue/checkpoint\" \n",
    "    # Create and start the stream-static join job using the factory function\n",
    "    query = create_stream_static_join(stream_table_name, static_table_name, stream_join_column,static_join_column, output_table_name, output_checkpoint_path)\n",
    "    # spark.sql(\"create table if not exists eco.gold.geographic_revenue location 'abfss://gold@ecommerceproject.dfs.core.windows.net/joins/geographic_revenue/delta_table'\")\n",
    "    query.awaitTermination(30)\n",
    "    joined_stream_df = spark.readStream.format(\"delta\").load(\"abfss://gold@ecommerceproject.dfs.core.windows.net/joins/geographic_revenue/delta_table\")\n",
    "    return joined_stream_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "224432ee-6aa1-4192-807d-f03f5c52fa09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8992861733321017,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
