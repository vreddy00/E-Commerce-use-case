{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4db71e90-6a37-4ff2-b960-080f8a35cf41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead,col,broadcast,collect_set,size,array_contains,collect_list,min,datediff, avg,rank,when,count,coalesce,lit,desc,sum,countDistinct,udf,date_format,approx_count_distinct\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "import os\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.streaming import StreamingQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d5f401f-8ee4-4443-8ff4-e6edc6878d9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Bronze/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "552cd084-1905-4f4c-9a9e-baba8a16c538",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class IncrementalPreProcessor:\n",
    "    def __init__(self, bronze_table_path, silver_table_path):\n",
    "        self.bronze_table_path = bronze_table_path\n",
    "        self.silver_table_path = silver_table_path\n",
    "        self.checkpoint_path =  silver_table_path + \"/checkpoint/\"\n",
    "\n",
    "    def preprocess_stream(self):\n",
    "        # Step 1: Read from Bronze Delta table as a Stream\n",
    "        bronze_stream = (spark.readStream\n",
    "            .format(\"delta\")\n",
    "            .load(self.bronze_table_path))\n",
    "\n",
    "        # Step 2: Apply Preprocessing (drop columns, fill nulls, etc.)\n",
    "        processed_stream = (bronze_stream\n",
    "            .drop(\"order_approved_at\", \"order_delivered_carrier_date\", \"order_delivered_customer_date\", \"order_estimated_delivery_date\",\"_rescued_data\")\n",
    "            .fillna({\"order_status\": \"unavailable\"}) \n",
    "        )\n",
    "\n",
    "        # Step 3: Write preprocessed data to Silver Delta Table\n",
    "        query = (processed_stream.writeStream\n",
    "            .format(\"delta\")\n",
    "            .outputMode(\"append\")\n",
    "            # .queryName(\"Preprocessing Stream data\")\n",
    "            .option(\"checkpointLocation\", self.checkpoint_path)\n",
    "            .start(self.silver_table_path))\n",
    "\n",
    "        return query\n",
    "\n",
    "# Paths for Delta tables\n",
    "# bronze_table_path = \"abfss://bronze@ecommerceproject2.dfs.core.windows.net/orders/delta_table\"\n",
    "# silver_table_path = \"abfss://silver@ecommerceproject2.dfs.core.windows.net/orders\"\n",
    "\n",
    "# # Start Streaming Preprocessing\n",
    "# processor = IncrementalPreProcessor(bronze_table_path, silver_table_path)\n",
    "# query = processor.preprocess_stream()\n",
    "# query.awaitTermination(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2118833-3702-4910-9767-187ffac31431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class PreProcessPipeline:\n",
    "    def __init__(self, storage_base_path: str, is_incremental: bool):\n",
    "        self.storage_base_path = storage_base_path.rstrip('/')  \n",
    "        self.is_incremental = is_incremental \n",
    "\n",
    "    def preprocess_data(\n",
    "        self, df: DataFrame, drop_columns: list = None, fill_na_dict: dict = None, \n",
    "        drop_duplicates_columns: list = None, replace_dict: dict = None) -> DataFrame:\n",
    "        \n",
    "        if df is None:\n",
    "            print(\"⚠ Warning: Received NoneType DataFrame. Returning previously pre-processed data.\")\n",
    "            return self.previous_df if hasattr(self, 'previous_df') else None\n",
    "        \n",
    "        if drop_columns:\n",
    "            print(f\"Dropping columns: {drop_columns}\")\n",
    "            df = df.drop(*drop_columns)\n",
    "\n",
    "        if fill_na_dict:\n",
    "            print(f\"Filling nulls with: {fill_na_dict}\")\n",
    "            df = df.fillna(fill_na_dict)\n",
    "        \n",
    "        if drop_duplicates_columns:\n",
    "            print(f\"Dropping duplicate records based on columns: {drop_duplicates_columns}\")\n",
    "            df = df.dropDuplicates(drop_duplicates_columns)\n",
    "        \n",
    "        if replace_dict:\n",
    "            for column, replacements in replace_dict.items():\n",
    "                print(f\"Replacing values in column '{column}': {replacements}\")\n",
    "                df = df.replace(replacements, subset=[column])\n",
    "        \n",
    "        self.previous_df = df  # Store the latest processed DataFrame\n",
    "        return df\n",
    "    \n",
    "    def store_as_delta(self, df: DataFrame, table_name: str, file_name: str = None):\n",
    "        if df is None:\n",
    "            print(\"⚠ Warning: Received NoneType DataFrame. Skipping Delta storage.\")\n",
    "            return\n",
    "        \n",
    "        delta_table_path = f\"{self.storage_base_path}/{table_name}\"\n",
    "\n",
    "        if self.is_incremental:\n",
    "            df = df.withColumn(\"filename\", lit(file_name))  # Add filename column\n",
    "\n",
    "            if DeltaTable.isDeltaTable(spark, delta_table_path):\n",
    "                delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "                delta_table.alias(\"target\").merge(\n",
    "                    df.alias(\"source\"),\n",
    "                    \"target.filename = source.filename\"\n",
    "                ).whenNotMatchedInsertAll().execute()\n",
    "                print(f\"✔ File '{file_name}' appended to Delta table at {delta_table_path}\")\n",
    "            else:\n",
    "                df.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"filename\").save(delta_table_path)\n",
    "                print(f\"Delta table created at {delta_table_path} with first file: {file_name}\")\n",
    "        else:\n",
    "            if DeltaTable.isDeltaTable(spark, delta_table_path):\n",
    "                print(f\"⚠ Delta table already exists at {delta_table_path}. Skipping overwrite.\")\n",
    "            else:\n",
    "                df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "                print(f\"Data stored as a new Delta table at {delta_table_path}\")\n",
    "\n",
    "    def run_pipeline(\n",
    "        self, df: DataFrame, table_name: str, drop_columns: list = None, \n",
    "        fill_na_dict: dict = None, drop_duplicates_columns: list = None, replace_dict: dict = None\n",
    "    ):\n",
    "        print(f\"Starting PreProcess pipeline for table: {table_name}\")\n",
    "        \n",
    "        if df is None:\n",
    "            print(\"⚠ Warning: Received NoneType DataFrame. Using previously pre-processed data.\")\n",
    "            df = self.previous_df if hasattr(self, 'previous_df') else None\n",
    "            if df is None:\n",
    "                print(\"No previously pre-processed data available. Skipping pipeline execution.\")\n",
    "                return\n",
    "        \n",
    "        file_name = \"Single_Batch\"\n",
    "        if self.is_incremental:\n",
    "            try:\n",
    "                file_path = df.inputFiles()[0] if df.inputFiles() else \"Unknown_File\"\n",
    "            except AttributeError:\n",
    "                file_path = \"Unknown_File\"\n",
    "            \n",
    "            file_name = os.path.basename(file_path).split(\".\")[0]  # Extract filename without extension\n",
    "            if file_name == \"Unknown_File\":\n",
    "                print(\"⚠ Warning: Could not determine input file name!\")\n",
    "\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "        \n",
    "        processed_df = self.preprocess_data(df, drop_columns, fill_na_dict, drop_duplicates_columns, replace_dict)\n",
    "        self.store_as_delta(processed_df, table_name, file_name)\n",
    "        \n",
    "        print(f\"Pre-Processing completed for file: {file_name}\")\n",
    "\n",
    "storage_base_path = \"abfss://silver@ecommerceproject2.dfs.core.windows.net/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67c8b3cb-fef0-4c8d-a36b-97e470c227ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_order_items(inputDf: dict) -> DataFrame:\n",
    "    config = {\"table_name\": \"order_items\",                  \n",
    "        \"drop_columns\": [\"shipping_limit_date\"],\n",
    "        \"fill_na_dict\": {\"price\": 0.0, \"freight_value\": 0.0},\n",
    "        \"drop_duplicates_columns\": [],\n",
    "        \"replace_dict\": {}}\n",
    "    pipeline = PreProcessPipeline(storage_base_path, is_incremental=True)\n",
    "    pipeline.run_pipeline(inputDf.get(\"order_itemsDf\"), config[\"table_name\"], config[\"drop_columns\"], \n",
    "                          config[\"fill_na_dict\"], drop_duplicates_columns=config[\"drop_duplicates_columns\"], \n",
    "                          replace_dict=config[\"replace_dict\"])\n",
    "    delta_table_path = f\"{storage_base_path}/{config['table_name']}\"\n",
    "    print(f\"Loading preprocessed table from Delta path: {delta_table_path}\")\n",
    "    return spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2baa2ed0-3888-4c11-bf53-3ad3111c56ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_sellers(inputDf: dict) -> DataFrame:\n",
    "    config = {\n",
    "        \"table_name\": \"sellers\",                  \n",
    "        \"drop_columns\": [],\n",
    "        \"fill_na_dict\": {},\n",
    "        \"drop_duplicates_columns\": [],\n",
    "        \"replace_dict\": {} }\n",
    "    \n",
    "    pipeline = PreProcessPipeline(storage_base_path, is_incremental=False)\n",
    "    \n",
    "    pipeline.run_pipeline(\n",
    "        inputDf.get(\"sellerDf\"), \n",
    "        config[\"table_name\"], \n",
    "        config[\"drop_columns\"], \n",
    "        config[\"fill_na_dict\"], \n",
    "        drop_duplicates_columns=config[\"drop_duplicates_columns\"], \n",
    "        replace_dict=config[\"replace_dict\"]\n",
    "    )\n",
    "\n",
    "    delta_table_path = f\"{storage_base_path}/{config['table_name']}\"\n",
    "    print(f\"Loading preprocessed table from Delta path: {delta_table_path}\")\n",
    "    \n",
    "    return spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13dc6137-5ecd-4e19-b976-3195f538fb0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_payments(inputDf: dict) -> DataFrame:\n",
    "    config = {\n",
    "        \"table_name\": \"payment\",                  \n",
    "        \"drop_columns\": [\"payment_sequential\", \"payment_value\"],\n",
    "        \"fill_na_dict\": {},\n",
    "        \"drop_duplicates_columns\": [], \n",
    "        \"replace_dict\": {\"payment_type\": {\"boleto\": \"cash\"}}}\n",
    "    \n",
    "    pipeline = PreProcessPipeline(storage_base_path, is_incremental=False)\n",
    "    \n",
    "    pipeline.run_pipeline(\n",
    "        inputDf.get(\"paymentDf\"), \n",
    "        config[\"table_name\"], \n",
    "        config[\"drop_columns\"], \n",
    "        config[\"fill_na_dict\"],\n",
    "        drop_duplicates_columns=config[\"drop_duplicates_columns\"],  \n",
    "        replace_dict=config[\"replace_dict\"])\n",
    "\n",
    "    delta_table_path = f\"{storage_base_path}/{config['table_name']}\"\n",
    "    print(f\"Loading preprocessed table from Delta path: {delta_table_path}\")\n",
    "    \n",
    "    return spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44d5e523-fef5-4190-b660-84e570d520d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_products(inputDf: dict) -> DataFrame:\n",
    "    config = {\n",
    "        \"table_name\": \"products\",                  \n",
    "        \"drop_columns\": [\n",
    "            \"product_name_lenght\", \"product_description_lenght\", \n",
    "            \"product_photos_qty\", \"product_weight_g\", \"product_length_cm\", \n",
    "            \"product_height_cm\", \"product_width_cm\"\n",
    "        ],\n",
    "        \"fill_na_dict\": {\"product_category_name\": \"unknown\"},\n",
    "        \"drop_duplicates_columns\": [],\n",
    "        \"replace_dict\": {}\n",
    "    }\n",
    "    \n",
    "    pipeline = PreProcessPipeline(storage_base_path, is_incremental=False)\n",
    "    \n",
    "    pipeline.run_pipeline(\n",
    "        inputDf.get(\"products_Df\"), \n",
    "        config[\"table_name\"], \n",
    "        config[\"drop_columns\"], \n",
    "        config[\"fill_na_dict\"], \n",
    "        drop_duplicates_columns=config[\"drop_duplicates_columns\"], \n",
    "        replace_dict=config[\"replace_dict\"]\n",
    "    )\n",
    "    \n",
    "    delta_table_path = f\"{storage_base_path}/{config['table_name']}\"\n",
    "    print(f\"Loading preprocessed table from Delta path: {delta_table_path}\")\n",
    "    \n",
    "    return spark.read.format(\"delta\").load(delta_table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28169f67-9f79-4115-ac3a-51d3631e37da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_customers(inputDf: dict) -> DataFrame:\n",
    "    config = {\n",
    "        \"table_name\": \"customers\",                  \n",
    "        \"drop_columns\": [\"customer_unique_id\"],\n",
    "        \"fill_na_dict\": {},\n",
    "        \"drop_duplicates_columns\": [],\n",
    "        \"replace_dict\": {}\n",
    "    }\n",
    "    \n",
    "    pipeline = PreProcessPipeline(storage_base_path, is_incremental=False)\n",
    "    \n",
    "    pipeline.run_pipeline(\n",
    "        inputDf.get(\"customerDf\"), \n",
    "        config[\"table_name\"], \n",
    "        config[\"drop_columns\"], \n",
    "        config[\"fill_na_dict\"], \n",
    "        drop_duplicates_columns=config[\"drop_duplicates_columns\"], \n",
    "        replace_dict=config[\"replace_dict\"]\n",
    "    )\n",
    "    \n",
    "    delta_table_path = f\"{storage_base_path}/{config['table_name']}\"\n",
    "    print(f\"Loading preprocessed table from Delta path: {delta_table_path}\")\n",
    "    \n",
    "    return spark.read.format(\"delta\").load(delta_table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb253171-2d7e-433b-8c25-91bb50a150fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_product_category(inputDf: dict) -> DataFrame:\n",
    "    config = {\n",
    "        \"table_name\": \"product_category\",                  \n",
    "        \"drop_columns\": [],\n",
    "        \"fill_na_dict\": {},\n",
    "        \"drop_duplicates_columns\": [],\n",
    "        \"replace_dict\": {}\n",
    "    }\n",
    "    \n",
    "    pipeline = PreProcessPipeline(storage_base_path, is_incremental=False)\n",
    "    \n",
    "    pipeline.run_pipeline(\n",
    "        inputDf.get(\"categroy_translation_Df\"), \n",
    "        config[\"table_name\"], \n",
    "        config[\"drop_columns\"], \n",
    "        config[\"fill_na_dict\"], \n",
    "        drop_duplicates_columns=config[\"drop_duplicates_columns\"], \n",
    "        replace_dict=config[\"replace_dict\"]\n",
    "    )\n",
    "    \n",
    "    delta_table_path = f\"{storage_base_path}/{config['table_name']}\"\n",
    "    print(f\"Loading preprocessed table from Delta path: {delta_table_path}\")\n",
    "    \n",
    "    return spark.read.format(\"delta\").load(delta_table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e805230-e23e-4979-94dc-6cee9be0cb87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_geolocation(inputDf: dict) -> DataFrame:\n",
    "    config = {\n",
    "        \"table_name\": \"geolocation_cleaned\",\n",
    "        \"drop_columns\": [\"geolocation_lat\", \"geolocation_lng\"], \n",
    "        \"fill_na_dict\": {},  \n",
    "        \"drop_duplicates_columns\": [\"geolocation_zip_code_prefix\"],\n",
    "        \"replace_dict\": {\"geolocation_city\": {\n",
    "                'á': 'a', 'à': 'a', 'ã': 'a', 'â': 'a', 'ä': 'a',\n",
    "                'é': 'e', 'è': 'e', 'ê': 'e',\n",
    "                'í': 'i', 'ì': 'i', 'î': 'i',\n",
    "                'ó': 'o', 'ò': 'o', 'ô': 'o', 'õ': 'o', 'ö': 'o',\n",
    "                'ú': 'u', 'ù': 'u', 'ü': 'u',\n",
    "                'ç': 'c', 'ñ': 'n'}}}\n",
    "    pipeline = PreProcessPipeline(storage_base_path, is_incremental=False)\n",
    "    pipeline.run_pipeline(\n",
    "        inputDf.get(\"geolocationDf\"), \n",
    "        config[\"table_name\"], \n",
    "        config[\"drop_columns\"], \n",
    "        config[\"fill_na_dict\"], \n",
    "        drop_duplicates_columns=config[\"drop_duplicates_columns\"], \n",
    "        replace_dict=config[\"replace_dict\"])\n",
    "    delta_table_path = f\"{storage_base_path}/{config['table_name']}\"\n",
    "    print(f\"Loading preprocessed table from Delta path: {delta_table_path}\")\n",
    "    return spark.read.format(\"delta\").load(delta_table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c70839b7-384b-4bff-a9c9-80d41cbbb6a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# class Transformer:\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "#     def transform(self, inputDf):\n",
    "#         raise NotImplementedError(\"transform() not implemented\")\n",
    "\n",
    "# class TotalRevenueTransformer(Transformer):\n",
    "#     \"\"\"Total revenue from orders\"\"\"\n",
    "#     def transform(self, preprocessed_df: DataFrame):\n",
    "#         transformed_df = preprocessed_df.withColumn(\"Item_Total\", col(\"price\") + col(\"freight_value\"))\n",
    "#         revenue_df = transformed_df.groupby(\"order_id\").agg(sum(\"Item_Total\").alias(\"Total_price\"))\n",
    "#         print(\"Total revenue per order:\")\n",
    "#         rdf=revenue_df.select(sum(\"Total_price\").alias(\"total_revenue\"))\n",
    "#         # rdf.display()\n",
    "#         return rdf\n",
    "    \n",
    "# class RevenueByProductCategoryTransformer(Transformer):\n",
    "#     \"\"\"Total revenue from each product category\"\"\"\n",
    "#     def transform(self, joined_df: DataFrame):\n",
    "#         transformed_df = joined_df.withColumn(\"Item_Total\", col(\"price\") + col(\"freight_value\"))\n",
    "#         revenue_df = transformed_df.groupby(\"product_category_name_english\").agg(sum(\"Item_Total\").alias(\"Category_Revenue\"))\n",
    "#         print(\"Total revenue per category:\")\n",
    "#         rdf=revenue_df.orderBy(\"Category_Revenue\", ascending=False)\n",
    "#         rdf.display()\n",
    "#         return rdf\n",
    "    \n",
    "# class TopSellingProduct(Transformer):\n",
    "#     \"\"\"Top Selling Products\"\"\"\n",
    "#     def transform(self, preprocessed_df: DataFrame):\n",
    "#         # Use the same preprocessed table.\n",
    "#         transformed_df = preprocessed_df.withColumn(\"Item_Total\", col(\"price\") + col(\"freight_value\"))\n",
    "#         result_df = transformed_df.groupby(\"product_id\").agg(count(\"order_item_id\").alias(\"order_count\"))\n",
    "#         print(\"Top Selling Products (by order count):\")\n",
    "#         rdf=result_df.orderBy(desc(\"order_count\"))\n",
    "#         rdf.display()\n",
    "#         return rdf\n",
    "\n",
    "# class RevenueTrendOverTimeTransformer(Transformer):\n",
    "#     \"\"\"Daily revenue from orders\"\"\"\n",
    "#     def transform(self, joined_df: DataFrame):\n",
    "#         transformed_df = joined_df.withColumn(\"Item_Total\", col(\"price\") + col(\"freight_value\"))\n",
    "#         revenue_df = transformed_df.groupby(date_format(\"order_purchase_timestamp\", \"yyyy-MM-dd\").alias(\"date\")).agg(sum(\"Item_Total\").alias(\"Daily_Revenue\"))\n",
    "#         print(\"Daily revenue :\")\n",
    "#         rdf=revenue_df.orderBy(\"Daily_Revenue\", ascending=False)\n",
    "#         rdf.display()\n",
    "#         return rdf\n",
    "    \n",
    "# class GeographicRevenueTransformer(Transformer):\n",
    "#     \"\"\"State wise revenue from orders\"\"\"\n",
    "#     def transform(self, joined_df: DataFrame):\n",
    "#         transformed_df = joined_df.withColumn(\"Item_Total\", col(\"price\") + col(\"freight_value\"))\n",
    "#         revenue_df = transformed_df.groupby(\"customer_state\").agg(sum(\"Item_Total\").alias(\"State_Revenue\"))\n",
    "#         print(\"State wise revenue :\")\n",
    "#         rdf=revenue_df.orderBy(\"State_Revenue\", ascending=False)\n",
    "#         rdf.display()\n",
    "#         return rdf\n",
    "    \n",
    "# class CustomerValueTransformer(Transformer):\n",
    "#     \"\"\"State wise revenue from orders\"\"\"\n",
    "#     def transform(self, joined_df: DataFrame):\n",
    "#         transformed_df = joined_df.withColumn(\"Item_Total\", col(\"price\") + col(\"freight_value\"))\n",
    "#         revenue_df = transformed_df.groupby(\"customer_id\").agg(sum(\"Item_Total\").alias(\"Customer_Spent\"))\n",
    "#         revenue_df = revenue_df.withColumn(\n",
    "#             \"Customer_Category\",\n",
    "#             when((col(\"Customer_Spent\") >= 5001), \"High\")\n",
    "#             .when((col(\"Customer_Spent\") >= 2001) & (col(\"Customer_Spent\") <= 5000), \"Mid\")\n",
    "#             .when((col(\"Customer_Spent\") >= 0) & (col(\"Customer_Spent\") <= 2000), \"Low\")\n",
    "#             .otherwise(\"Unknown\")\n",
    "#         )\n",
    "#         print(\"Customer Revenue and Category:\")\n",
    "#         rdf=revenue_df.orderBy(col(\"Customer_Spent\").desc())\n",
    "#         rdf.display()\n",
    "#         return rdf\n",
    "    \n",
    "# class AverageOrderValueTransformer(Transformer):\n",
    "#     \"\"\"Calculates Average Order Value (AOV)\"\"\"\n",
    "#     def transform(self, order_items):\n",
    "#         order_items = order_items.withColumn(\"Item_Total\", col(\"price\") + col(\"freight_value\"))\n",
    "#         order_totals_df = order_items.groupBy(\"order_id\").agg(sum(\"Item_Total\").alias(\"order_total\"))\n",
    "#         aov_df = order_totals_df.selectExpr(\"SUM(order_total) / COUNT(order_id) AS AOV\")\n",
    "#         aov_df.display()\n",
    "#         return aov_df\n",
    "\n",
    "# class OrderReturnRateTransformer(Transformer):\n",
    "#     \"\"\"Calculates Order Return Rate\"\"\"\n",
    "#     def transform(self, orders_df):\n",
    "#         return_rate_df = orders_df.agg(\n",
    "#             (sum(when(col(\"order_status\") == \"canceled\", 1).otherwise(0)) / count(\"order_id\"))\n",
    "#             .alias(\"Order_Return_Rate\")\n",
    "#         )\n",
    "#         return_rate_df.display()\n",
    "#         return return_rate_df\n",
    "    \n",
    "# class CustomerRetentionTransformer(Transformer):\n",
    "#     def transform(self, joined_df):\n",
    "#         cust_retention = (joined_df.groupby(\"customer_id\")\n",
    "#                           .agg(count(\"order_id\").alias(\"total_orders\"))\n",
    "#                           .filter(\"total_orders > 1\")\n",
    "#                           .orderBy(\"total_orders\", ascending=False))\n",
    "#         cust_retention.display()            \n",
    "#         return cust_retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22fe7c11-22a7-4c19-8c11-1413ef1b97c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_stream_static_join(stream_table_name: DataFrame, static_table_name: DataFrame, stream_join_column: str, static_join_column: str,output_table_name: str, output_checkpoint_path: str) -> 'StreamingQuery':\n",
    "    # Step 1: Read streaming data from Unity Catalog\n",
    "    stream_df =stream_table_name\n",
    "\n",
    "    # Step 2: Read static batch data from Unity Catalog\n",
    "    static_df = static_table_name\n",
    "\n",
    "    # Step 3: Perform the join (default shuffle join)\n",
    "    joined_stream = (stream_df\n",
    "        .join(static_df, stream_df[stream_join_column] == static_df[static_join_column], \"left\") ) # Left join\n",
    "\n",
    "    # Step 4: Select all columns from stream_df and all columns from static_df except the join column\n",
    "    selected_columns = [stream_df[col] for col in stream_df.columns] + \\\n",
    "                       [static_df[col] for col in static_df.columns if col != static_join_column]\n",
    "    joined_stream = joined_stream.select(*selected_columns)\n",
    "\n",
    "    # Step 4: Write the result to a Delta table with checkpointing\n",
    "    query = (joined_stream.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", output_checkpoint_path)\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .start(output_table_name) )\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8736fa87-af7d-4eef-8dba-75c4d3ca11aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def order_items_join(order_df: DataFrame, order_items_df: DataFrame) -> DataFrame:\n",
    "    stream_table_name =order_df\n",
    "    static_table_name = order_items_df\n",
    "    stream_join_column = \"order_id\" \n",
    "    static_join_column = \"order_id\" \n",
    "    output_table_name = \"abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/orders_items_join/delta_table\" \n",
    "    output_checkpoint_path = \"abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/orders_items_join/checkpoint\" \n",
    "    query = create_stream_static_join(stream_table_name, static_table_name, stream_join_column,static_join_column, output_table_name, output_checkpoint_path)\n",
    "    spark.sql(\"create catalog if not exists eco\")\n",
    "    spark.sql(\"create schema if not exists eco.silver\")\n",
    "    spark.sql(\"create table if not exists eco.silver.orders_join location 'abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/orders_items_join/delta_table'\")\n",
    "    query.awaitTermination(20)\n",
    "    joined_stream_df = spark.readStream.format(\"delta\").load(\"abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/orders_items_join/delta_table\")\n",
    "    return joined_stream_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fa6ccb9-55f9-44f1-8b9c-6e9dfb2a4f56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def order_payments_join(order_df: DataFrame, order_payments) -> DataFrame:\n",
    "    stream_table_name =order_df\n",
    "    static_table_name = order_payments\n",
    "    stream_join_column = \"order_id\" \n",
    "    static_join_column = \"order_id\" \n",
    "    output_table_name = \"abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/order_payments_join/delta_table\" \n",
    "    output_checkpoint_path = \"abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/order_payments_join/checkpoint\" \n",
    "    query = create_stream_static_join(stream_table_name, static_table_name, stream_join_column,static_join_column, output_table_name, output_checkpoint_path)\n",
    "    spark.sql(\"create catalog if not exists eco\")\n",
    "    spark.sql(\"create schema if not exists eco.silver\")\n",
    "    spark.sql(\"create table if not exists eco.silver.order_payments location 'abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/order_payments_join/delta_table'\")\n",
    "    query.awaitTermination(20)\n",
    "    joined_stream_df = spark.readStream.format(\"delta\").load(\"abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/order_payments_join/delta_table\")\n",
    "    return joined_stream_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9936bb0-9e69-4d98-96c9-67a49abfedee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def order_seller_join(orders:DataFrame,order_items:DataFrame,sellers) -> DataFrame:\n",
    "    stream_table_name = order_items_join(orders,order_items)\n",
    "    static_table_name = sellers\n",
    "    stream_join_column = \"seller_id\" \n",
    "    static_join_column = \"seller_id\" \n",
    "    output_table_name = \"abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/order_seller/delta_table\"  \n",
    "    output_checkpoint_path = \"abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/order_seller/checkpoint\" \n",
    "    query = create_stream_static_join(stream_table_name, static_table_name,stream_join_column,static_join_column, output_table_name, output_checkpoint_path)\n",
    "    spark.sql(\"create table if not exists eco.silver.order_seller location 'abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/order_seller/delta_table'\")\n",
    "    query.awaitTermination(20)\n",
    "    joined_stream_df = spark.readStream.format(\"delta\").load(\"abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/order_seller/delta_table\")\n",
    "    return joined_stream_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "195c4ae0-d91e-41b1-903d-e1b0988d85b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def category_revenue_join(orders:DataFrame,order_items:DataFrame,products,product_categories) -> DataFrame:\n",
    "    stream_table_name = order_items_join(orders,order_items)\n",
    "    joined_df=products.join(product_categories, products.product_category_name == product_categories.product_category_name).select(products[\"product_id\"],products[\"product_name\"],product_categories[\"product_category_name_english\"])\n",
    "    static_table_name = joined_df \n",
    "    stream_join_column = \"product_id\" \n",
    "    static_join_column = \"product_id\" \n",
    "    output_table_name = \"abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/category_revenue/delta_table\"  \n",
    "    output_checkpoint_path = \"abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/category_revenue/checkpoint\" \n",
    "    query = create_stream_static_join(stream_table_name, static_table_name,stream_join_column,static_join_column, output_table_name, output_checkpoint_path)\n",
    "    spark.sql(\"create table if not exists eco.silver.category_revenue location 'abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/category_revenue/delta_table'\")\n",
    "    query.awaitTermination(20)\n",
    "    joined_stream_df = spark.readStream.format(\"delta\").load(\"abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/category_revenue/delta_table\")\n",
    "    return joined_stream_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b75bca96-0954-4266-aa9c-3d0748b50647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# source_path = \"abfss://stream-data@ecommerceproject2.dfs.core.windows.net/\"\n",
    "# bronze_table_path = \"abfss://bronze@ecommerceproject2.dfs.core.windows.net/orders\"\n",
    "# delta_table_path=\"abfss://bronze@ecommerceproject2.dfs.core.windows.net/orders/delta_table/\"\n",
    "# silver_table_path = \"abfss://silver@ecommerceproject2.dfs.core.windows.net/orders/\"\n",
    "\n",
    "# extractor = Order_items()\n",
    "# orderitemDf = extractor.extract()\n",
    "# extractor = Products_Category()\n",
    "# productcatDf = extractor.extract()\n",
    "# extractor = Products()\n",
    "# productDf = extractor.extract()\n",
    "# extractor = StreamingProcessor(source_path, bronze_table_path)\n",
    "# orderDf = extractor.start_streaming_job()\n",
    "# orderDf.awaitTermination(30)\n",
    "\n",
    "# #Tranformation: Preprocessing\n",
    "# preprocessed_products_df = get_preprocessed_products(productDf)\n",
    "# preprocessed_product_category_df = get_preprocessed_product_category(productcatDf)\n",
    "# preprocessed_order_items_df = get_preprocessed_order_items(orderitemDf)\n",
    "# preprocessed_order_df=IncrementalPreProcessor(delta_table_path, silver_table_path)\n",
    "# query=preprocessed_order_df.preprocess_stream()\n",
    "# query.awaitTermination(30)\n",
    "# preprocessed_order_df=spark.readStream.format(\"delta\").load(silver_table_path)\n",
    "\n",
    "# #Tranformation: Joins\n",
    "# joined_df = category_revenue_join(preprocessed_order_df,preprocessed_order_items_df,preprocessed_products_df,preprocessed_product_category_df)\n",
    "# stream_joindf=spark.readStream.format(\"delta\").load(\"abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/category_revenue/delta_table\")\n",
    "\n",
    "# stream_joindf.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0835ab05-9772-46ec-b1c3-544a6f429509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def customer_spending_join(orders:DataFrame,order_items:DataFrame,customers:DataFrame) -> DataFrame:\n",
    "    stream_table_name = order_items_join(orders,order_items) \n",
    "    static_table_name = customers  \n",
    "    stream_join_column = \"customer_id\" \n",
    "    static_join_column = \"customer_id\" \n",
    "    output_table_name = \"abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/customer_spending/delta_table\"  \n",
    "    output_checkpoint_path = \"abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/customer_spending/checkpoint\" \n",
    "    query = create_stream_static_join(stream_table_name, static_table_name,stream_join_column,static_join_column, output_table_name, output_checkpoint_path) \n",
    "    spark.sql(\"create table if not exists eco.silver.customer_spending location 'abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/customer_spending/delta_table'\")\n",
    "    query.awaitTermination(20)\n",
    "    joined_stream_df = spark.readStream.format(\"delta\").load(\"abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/customer_spending/delta_table\")\n",
    "    return joined_stream_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "168c1737-a6e3-418b-9107-a8d74e568254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def geographic_revenue_join(orders,order_items,customers,geolocation):\n",
    "    stream_table_name = customer_spending_join(orders,order_items,customers)\n",
    "    static_table_name = geolocation  \n",
    "    stream_join_column = \"customer_zip_code_prefix\" \n",
    "    static_join_column = \"geolocation_zip_code_prefix\" \n",
    "    output_table_name = \"abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/geographic_revenue/delta_table\"  \n",
    "    output_checkpoint_path = \"abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/geographic_revenue/checkpoint\" \n",
    "    # Create and start the stream-static join job using the factory function\n",
    "    query = create_stream_static_join(stream_table_name, static_table_name, stream_join_column,static_join_column, output_table_name, output_checkpoint_path)\n",
    "    spark.sql(\"create table if not exists eco.silver.geo_revenue location 'abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/geographic_revenue/delta_table'\")\n",
    "    query.awaitTermination(20)\n",
    "    joined_stream_df = spark.readStream.format(\"delta\").load(\"abfss://gold@ecommerceproject2.dfs.core.windows.net/joins/geographic_revenue/delta_table\")\n",
    "    return joined_stream_df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8013436609082931,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver (2) (1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
